{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5033146-a51e-4837-8c52-63d279a3719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5d70f2-0d73-4197-afcc-6b0826748c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('/home/jovyan/Sample_Based_Extension/UNSW/x_test.npy')\n",
    "x_train = np.load('/home/jovyan/Sample_Based_Extension/UNSW/x_train.npy')\n",
    "x_val = np.load('/home/jovyan/Sample_Based_Extension/UNSW/x_val.npy')\n",
    "y_test = np.load('/home/jovyan/Sample_Based_Extension/UNSW/y_test.npy')\n",
    "y_train = np.load('/home/jovyan/Sample_Based_Extension/UNSW/y_train.npy')\n",
    "y_val = np.load('/home/jovyan/Sample_Based_Extension/UNSW/y_val.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b531ff-ca2b-4d0a-8f97-8602de5d4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 50\n",
    "training_batch_size = 128\n",
    "validation_batch_size = 128\n",
    "learning_rate = 0.01\n",
    "baseline_temperature = 1\n",
    "distilled_temperature = 100\n",
    "\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = len(np.unique(y_train))\n",
    "\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "        self.fc3 = nn.Linear(30, 20)\n",
    "        self.fc4 = nn.Linear(20, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models\n",
    "teacher_model = DNNModel(input_shape, output_shape).to(device)\n",
    "student_model = DNNModel(input_shape, output_shape).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fca48ce-e435-4b70-8ef0-51d6ba296dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for the teacher model\n",
    "def train_teacher_model(model, train_loader, epochs, temperature):\n",
    "    min_delta = 0.001\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    best_loss = float('100000000')\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs / temperature, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_train_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_val_loss = val_train_loss / len(val_loader)\n",
    "        val_accuracy = correct_predictions / len(val_dataset)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "        # Early stopping check using min_delta\n",
    "        if best_loss - avg_val_loss > min_delta:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "    \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    torch.save(model.state_dict(), 'teacher_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d6cb94-44c9-4639-b78b-c07ecf5c06b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.0000, Validation Loss: 0.1205, Validation Accuracy: 0.9497\n",
      "Epoch 2, Training Loss: 0.0000, Validation Loss: 0.1200, Validation Accuracy: 0.9481\n",
      "Epoch 3, Training Loss: 0.0000, Validation Loss: 0.1166, Validation Accuracy: 0.9483\n",
      "Epoch 4, Training Loss: 0.0000, Validation Loss: 0.0965, Validation Accuracy: 0.9608\n",
      "Epoch 5, Training Loss: 0.0000, Validation Loss: 0.1026, Validation Accuracy: 0.9546\n",
      "Epoch 6, Training Loss: 0.0000, Validation Loss: 0.1073, Validation Accuracy: 0.9548\n",
      "Epoch 7, Training Loss: 0.0000, Validation Loss: 0.0940, Validation Accuracy: 0.9609\n",
      "Epoch 8, Training Loss: 0.0000, Validation Loss: 0.0914, Validation Accuracy: 0.9627\n",
      "Epoch 9, Training Loss: 0.0000, Validation Loss: 0.1029, Validation Accuracy: 0.9577\n",
      "Epoch 10, Training Loss: 0.0000, Validation Loss: 0.0999, Validation Accuracy: 0.9585\n",
      "Epoch 11, Training Loss: 0.0000, Validation Loss: 0.0960, Validation Accuracy: 0.9623\n",
      "Epoch 12, Training Loss: 0.0000, Validation Loss: 0.0936, Validation Accuracy: 0.9600\n",
      "Epoch 13, Training Loss: 0.0000, Validation Loss: 0.0973, Validation Accuracy: 0.9600\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Train the teacher model\n",
    "train_teacher_model(teacher_model, train_loader, epochs, baseline_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7ea4a-63ec-46f0-9132-b9d20bfa7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_model.load_state_dict(torch.load(\"/home/jovyan/Sample_Based_Extension/UNSW/UNSW_Defense/Defense_Distillation/teacher_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe75af1-670e-4616-bd84-fc328f39b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation loss function\n",
    "def distillation_loss(student_outputs, teacher_outputs, temperature, alpha):\n",
    "    soft_targets = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    student_log_probs = F.log_softmax(student_outputs / temperature, dim=1)\n",
    "    distillation_loss = nn.KLDivLoss()(student_log_probs, soft_targets) * (temperature ** 2)\n",
    "    return distillation_loss\n",
    "\n",
    "# Training function for the student model using distillation\n",
    "def train_student_model(student_model, teacher_model, train_loader, epochs, temperature, alpha=0.5):\n",
    "    min_delta = 0.001\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    best_loss = float('100000000')\n",
    "    \n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            teacher_outputs = teacher_model(inputs).detach()\n",
    "            student_outputs = student_model(inputs)\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, temperature, alpha) + criterion(student_outputs, labels) * (1. - alpha)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        student_model.eval()\n",
    "        val_train_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                teacher_outputs = teacher_model(inputs).detach()\n",
    "                student_outputs = student_model(inputs)\n",
    "                loss = distillation_loss(student_outputs, teacher_outputs, temperature, alpha) + criterion(student_outputs, labels) * (1. - alpha)\n",
    "                val_train_loss += loss.item()\n",
    "                _, predicted = torch.max(student_outputs.data, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "        avg_val_loss = val_train_loss / len(val_loader)\n",
    "        val_accuracy = correct_predictions / len(val_dataset)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "        # Early stopping check using min_delta\n",
    "        if best_loss - avg_val_loss > min_delta:\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "    \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    torch.save(student_model.state_dict(), 'student_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a9de13-2bd6-4537-bf9f-f2bddb8f6023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.0000, Validation Loss: 0.5650, Validation Accuracy: 0.9501\n",
      "Epoch 2, Training Loss: 0.0000, Validation Loss: 0.3391, Validation Accuracy: 0.9538\n",
      "Epoch 3, Training Loss: 0.0000, Validation Loss: 0.2772, Validation Accuracy: 0.9456\n",
      "Epoch 4, Training Loss: 0.0000, Validation Loss: 0.1846, Validation Accuracy: 0.9519\n",
      "Epoch 5, Training Loss: 0.0000, Validation Loss: 0.3187, Validation Accuracy: 0.9507\n",
      "Epoch 6, Training Loss: 0.0000, Validation Loss: 0.2434, Validation Accuracy: 0.9542\n",
      "Epoch 7, Training Loss: 0.0000, Validation Loss: 0.2302, Validation Accuracy: 0.9508\n",
      "Epoch 8, Training Loss: 0.0000, Validation Loss: 0.1612, Validation Accuracy: 0.9600\n",
      "Epoch 9, Training Loss: 0.0000, Validation Loss: 0.1787, Validation Accuracy: 0.9593\n",
      "Epoch 10, Training Loss: 0.0000, Validation Loss: 0.1324, Validation Accuracy: 0.9622\n",
      "Epoch 11, Training Loss: 0.0000, Validation Loss: 0.1144, Validation Accuracy: 0.9634\n",
      "Epoch 12, Training Loss: 0.0000, Validation Loss: 0.1387, Validation Accuracy: 0.9611\n",
      "Epoch 13, Training Loss: 0.0000, Validation Loss: 0.1366, Validation Accuracy: 0.9588\n",
      "Epoch 14, Training Loss: 0.0000, Validation Loss: 0.1212, Validation Accuracy: 0.9620\n",
      "Epoch 15, Training Loss: 0.0000, Validation Loss: 0.1554, Validation Accuracy: 0.9519\n",
      "Epoch 16, Training Loss: 0.0000, Validation Loss: 0.1581, Validation Accuracy: 0.9610\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Train the student model\n",
    "train_student_model(student_model, teacher_model, train_loader, epochs, distilled_temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37639696-7815-41f0-999e-1c0fa7cdd076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student_model.load_state_dict(torch.load(\"/home/jovyan/Sample_Based_Extension/UNSW/UNSW_Defense/Defense_Distillation/student_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5c70d87-0e0f-4a4e-8c3e-a8d4de364a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_performance_metrics(X_test, y_true, model, model_name, attack_name, eps):\n",
    "#     model.eval()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "    \n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     probabilities = []\n",
    "\n",
    "#     num_classes = len(np.unique(y_true))\n",
    "    \n",
    "#     X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "#     y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "#     test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "#     test_loader = DataLoader(dataset=test_dataset)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             preds = torch.argmax(outputs, dim=1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "#         all_preds = np.array(all_preds)\n",
    "#         all_labels = np.array(all_labels)\n",
    "#         probabilities = np.array(probabilities)\n",
    "        \n",
    "#         accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "#         precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "#         precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "#         # macro_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='macro')\n",
    "#         # weighted_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='weighted')\n",
    "\n",
    "#         cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "#         def calculate_class_metrics_macro(cm, class_index):\n",
    "#             TP = cm[class_index, class_index]\n",
    "#             FP = cm[:, class_index].sum() - TP\n",
    "#             FN = cm[class_index, :].sum() - TP\n",
    "#             TN = cm.sum() - (TP + FP + FN)\n",
    "            \n",
    "#             TPR = TP / (TP + FN) if (TP + FN) != 0 else 0  \n",
    "#             TNR = TN / (TN + FP) if (TN + FP) != 0 else 0  \n",
    "#             FPR = FP / (FP + TN) if (FP + TN) != 0 else 0  \n",
    "#             FNR = FN / (FN + TP) if (FN + TP) != 0 else 0  \n",
    "            \n",
    "#             return TPR, TNR, FPR, FNR\n",
    "            \n",
    "#         metrics = np.array([calculate_class_metrics_macro(cm, i) for i in range(num_classes)])\n",
    "#         TPR_macro, TNR_macro, FPR_macro, FNR_macro = np.mean(metrics, axis=0)\n",
    "\n",
    "#         print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "#         print(\"\\nmacro\")\n",
    "#         print(f\"Precision: {precision_macro}\\nRecall: {recall_macro}\\nF1 Score: {f1_macro}\")\n",
    "    \n",
    "#         print(\"\\nweighted\")\n",
    "#         print(f\"Precision: {precision_weighted}\\nRecall: {recall_weighted}\\nF1 Score: {f1_weighted}\")\n",
    "#         print()\n",
    "        \n",
    "#         print(f\"Mean FNR: {FNR_macro}\\nMean TNR: {TNR_macro}\\nMean FPR: {FPR_macro}\\nMean TPR: {TPR_macro}\")\n",
    "\n",
    "#         new_row = {\n",
    "#             \"model\" : model_name,\n",
    "#             \"attack_model\" : attack_name,\n",
    "#             'epsilon': eps,\n",
    "#             'Accuracy': accuracy,\n",
    "#             'Macro Precision': precision_macro,\n",
    "#             'Weighted Precision': precision_weighted,\n",
    "#             'Macro Recall': recall_macro,\n",
    "#             'Weighted Recall': recall_weighted,\n",
    "#             'Macro F1': f1_macro,\n",
    "#             'Weighted F1': f1_weighted,\n",
    "#             # 'Macro AUC': macro_auc,\n",
    "#             # 'Weighted AUC': weighted_auc,\n",
    "#             'TPR': TPR_macro,\n",
    "#             'FNR': FNR_macro,\n",
    "#             'TNR': TNR_macro,\n",
    "#             'FPR': FPR_macro,\n",
    "#         }\n",
    "#         new_row_df = pd.DataFrame([new_row])\n",
    "#         new_row_df.to_csv(\"/home/jovyan/Sample_Based_Extension/UNSW/UNSW_Defense/Defense_Distillation/def_distillation.csv\", mode='a', index=False, header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7ca2d2-9805-4e8a-830d-0df7a6ce0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(X_test, y_true, model, model_name, attack_name, eps):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(dataset=test_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        probabilities = np.array(probabilities)\n",
    "\n",
    "        np.save(f\"/home/jovyan/Sample_Based_Extension/UNSW/UNSW_Defense_Label/UNSW_Def8/y_pred_{attack_name}{eps}_Def8.npy\", all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262ce9e9-ff34-44e0-a0e1-5aaba05d0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calculate_performance_metrics(x_test, y_test, student_model, 'DNN', 'Baseline', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9814807-09fa-4aa6-9633-70314c5a22c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start BIM\n",
      "start FGSM\n",
      "start PGD\n",
      "start DF\n"
     ]
    }
   ],
   "source": [
    "epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Iterate over epsilon values\n",
    "print(\"start BIM\")\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_BIM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'DD_BIM', epsilon)\n",
    "\n",
    "print(\"start FGSM\")\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_FGSM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'DD_FGSM', epsilon)\n",
    "\n",
    "print(\"start PGD\")\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_PGD_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'DD_PGD', epsilon)\n",
    "\n",
    "print(\"start DF\")\n",
    "# DF_eps = [0.01, 0.1, 0.2, 0.3]\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_DF_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'DD_DF', epsilon)\n",
    "# calculate_performance_metrics(x_test_adv, y_test_adv, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc9ef40-376c-4d40-91be-7bf2f2d38a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start AutoPGD\n",
      "start ZOO\n"
     ]
    }
   ],
   "source": [
    "epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Iterate over epsilon values\n",
    "print(\"start AutoPGD\")\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_AutoPGD_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'AutoPGD', epsilon)\n",
    "\n",
    "print(\"start ZOO\")\n",
    "ZOO_epsilon_values = [0, 0.01, 0.1, 0.2, 0.3]\n",
    "# Iterate over epsilon values\n",
    "for epsilon in ZOO_epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_ZOO_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'ZOO', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e78dc1b-0bce-4348-b4c9-35cf975fdeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start SINIFGSM\n"
     ]
    }
   ],
   "source": [
    "epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "print(\"start CaFA\")\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_CaFA_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'CaFA', epsilon)\n",
    "\n",
    "print(\"start SINIFGSM\")\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_SINIFGSM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'SINIFGSM', epsilon)\n",
    "\n",
    "\n",
    "print(\"start VNIFGSM\")\n",
    "# Iterate over epsilon values\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_VNIFGSM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'VNIFGSM', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f00e783-0ee7-40d3-a88c-216608106b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9043265392073422\n",
      "\n",
      "macro\n",
      "Precision: 0.8892049404404545\n",
      "Recall: 0.8913749684620725\n",
      "F1 Score: 0.8902752261431037\n",
      "\n",
      "weighted\n",
      "Precision: 0.9046155036432946\n",
      "Recall: 0.9043265392073422\n",
      "F1 Score: 0.9044595245412346\n",
      "\n",
      "Mean FNR: 0.10862503153792746\n",
      "Mean TNR: 0.8913749684620725\n",
      "Mean FPR: 0.10862503153792746\n",
      "Mean TPR: 0.8913749684620725\n",
      "Accuracy: 0.6825420501706575\n",
      "\n",
      "macro\n",
      "Precision: 0.6488128483335465\n",
      "Recall: 0.6620517194385973\n",
      "F1 Score: 0.6520832133731669\n",
      "\n",
      "weighted\n",
      "Precision: 0.7016680597230223\n",
      "Recall: 0.6825420501706575\n",
      "F1 Score: 0.6892702357855234\n",
      "\n",
      "Mean FNR: 0.3379482805614027\n",
      "Mean TNR: 0.6620517194385973\n",
      "Mean FPR: 0.3379482805614027\n",
      "Mean TPR: 0.6620517194385973\n",
      "Accuracy: 0.4582393766835423\n",
      "\n",
      "macro\n",
      "Precision: 0.4548687972012386\n",
      "Recall: 0.4481134265014356\n",
      "F1 Score: 0.43796973674416556\n",
      "\n",
      "weighted\n",
      "Precision: 0.5204191646390883\n",
      "Recall: 0.4582393766835423\n",
      "F1 Score: 0.47652644280330814\n",
      "\n",
      "Mean FNR: 0.5518865734985645\n",
      "Mean TNR: 0.4481134265014356\n",
      "Mean FPR: 0.5518865734985645\n",
      "Mean TPR: 0.4481134265014356\n",
      "Accuracy: 0.31644892121680074\n",
      "\n",
      "macro\n",
      "Precision: 0.3287092805280371\n",
      "Recall: 0.304049575780345\n",
      "F1 Score: 0.30194190908144825\n",
      "\n",
      "weighted\n",
      "Precision: 0.38944460157278643\n",
      "Recall: 0.31644892121680074\n",
      "F1 Score: 0.3382942034590866\n",
      "\n",
      "Mean FNR: 0.695950424219655\n",
      "Mean TNR: 0.304049575780345\n",
      "Mean FPR: 0.695950424219655\n",
      "Mean TPR: 0.304049575780345\n",
      "Execution Time: 707.058860 seconds\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# for epsilon in epsilon_values:\n",
    "#     filename = f'/home/jovyan/Sample_Based_Extension/UNSW/transfer_attack/x_test_adv_BIM_eps_{epsilon}.npy'\n",
    "#     x_test_adv = np.load(filename)\n",
    "\n",
    "#     calculate_performance_metrics(x_test_adv, y_test, student_model, 'DNN', 'BIM', epsilon)\n",
    "\n",
    "# end_time = time.time()\n",
    "# result = end_time - start_time\n",
    "# print(f\"Execution Time: {result:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f11e3-2789-4ace-9047-2f1039015221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
