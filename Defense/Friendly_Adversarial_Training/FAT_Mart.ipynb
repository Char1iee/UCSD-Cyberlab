{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddea530e-6be0-41c5-9465-167ce81f9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from art.attacks.evasion import SimBA, SpatialTransformation, DeepFool, BasicIterativeMethod, FastGradientMethod, ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac054ea5-7fdd-4616-856d-8a5f56d2d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = {\n",
    "            \"model\" : '',\n",
    "            \"attack_model\": '',\n",
    "            'epsilon': '',\n",
    "            'Accuracy': '',\n",
    "            'Macro Precision': '',\n",
    "            'Weighted Precision': '',\n",
    "            'Macro Recall': '',\n",
    "            'Weighted Recall': '',\n",
    "            'Macro F1': '',\n",
    "            'Weighted F1': '',\n",
    "            # 'Macro AUC': '',\n",
    "            # 'Weighted AUC': '',\n",
    "            'TPR': '',\n",
    "            'FNR': '',\n",
    "            'TNR': '',\n",
    "            'FPR': '',\n",
    "        }\n",
    "head = pd.DataFrame([head])\n",
    "head.to_csv(\"/home/jovyan/Defense/Friendly_Adversarial_Training/FAT_Mart.csv\", mode='a', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e4b2a1-406e-4f69-9a23-4f1a87c2633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(X_test, y_true, model, model_name, attack_name, eps):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(dataset=test_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        probabilities = np.array(probabilities)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "        # macro_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='macro')\n",
    "        # weighted_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='weighted')\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        def calculate_class_metrics_macro(cm, class_index):\n",
    "            TP = cm[class_index, class_index]\n",
    "            FP = cm[:, class_index].sum() - TP\n",
    "            FN = cm[class_index, :].sum() - TP\n",
    "            TN = cm.sum() - (TP + FP + FN)\n",
    "            \n",
    "            TPR = TP / (TP + FN) if (TP + FN) != 0 else 0  \n",
    "            TNR = TN / (TN + FP) if (TN + FP) != 0 else 0  \n",
    "            FPR = FP / (FP + TN) if (FP + TN) != 0 else 0  \n",
    "            FNR = FN / (FN + TP) if (FN + TP) != 0 else 0  \n",
    "            \n",
    "            return TPR, TNR, FPR, FNR\n",
    "            \n",
    "        metrics = np.array([calculate_class_metrics_macro(cm, i) for i in range(num_classes)])\n",
    "        TPR_macro, TNR_macro, FPR_macro, FNR_macro = np.mean(metrics, axis=0)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        print(\"\\nmacro\")\n",
    "        print(f\"Precision: {precision_macro}\\nRecall: {recall_macro}\\nF1 Score: {f1_macro}\")\n",
    "    \n",
    "        print(\"\\nweighted\")\n",
    "        print(f\"Precision: {precision_weighted}\\nRecall: {recall_weighted}\\nF1 Score: {f1_weighted}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Mean FNR: {FNR_macro}\\nMean TNR: {TNR_macro}\\nMean FPR: {FPR_macro}\\nMean TPR: {TPR_macro}\")\n",
    "\n",
    "        new_row = {\n",
    "            \"model\" : model_name,\n",
    "            \"attack_model\" : attack_name,\n",
    "            'epsilon': eps,\n",
    "            'Accuracy': accuracy,\n",
    "            'Macro Precision': precision_macro,\n",
    "            'Weighted Precision': precision_weighted,\n",
    "            'Macro Recall': recall_macro,\n",
    "            'Weighted Recall': recall_weighted,\n",
    "            'Macro F1': f1_macro,\n",
    "            'Weighted F1': f1_weighted,\n",
    "            # 'Macro AUC': macro_auc,\n",
    "            # 'Weighted AUC': weighted_auc,\n",
    "            'TPR': TPR_macro,\n",
    "            'FNR': FNR_macro,\n",
    "            'TNR': TNR_macro,\n",
    "            'FPR': FPR_macro,\n",
    "        }\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        new_row_df.to_csv(\"/home/jovyan/Defense/Friendly_Adversarial_Training/FAT_Mart.csv\", mode='a', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc5288c-f704-4c1e-8b20-f7df5fcf0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('/home/jovyan/Wustl_iiot/x_test.npy')\n",
    "x_train = np.load('/home/jovyan/Wustl_iiot/x_train.npy')\n",
    "x_val = np.load('/home/jovyan/Wustl_iiot/x_val.npy')\n",
    "y_test = np.load('/home/jovyan/Wustl_iiot/y_test.npy')\n",
    "y_train = np.load('/home/jovyan/Wustl_iiot/y_train.npy')\n",
    "y_val = np.load('/home/jovyan/Wustl_iiot/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4e66be-bf25-4a39-b3ef-4d0d8b807d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03857e0-e780-4fa8-96f3-e1399d7f2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1]\n",
    "output_shape = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f9ac046-77b7-4630-a4a8-fe44438ac801",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884d3dd2-ab96-48dc-85be-71553ba9454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "        self.fc3 = nn.Linear(30, 20)\n",
    "        self.fc4 = nn.Linear(20, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c47fa36-8689-4ec3-b66f-e0fc79b2d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earlystop(model, data, target, step_size, epsilon, perturb_steps,tau,randominit_type,loss_fn,rand_init=True,omega=0):\n",
    "    '''\n",
    "    The implematation of early-stopped PGD\n",
    "    Following the Alg.1 in our FAT paper <https://arxiv.org/abs/2002.11242>\n",
    "    :param step_size: the PGD step size\n",
    "    :param epsilon: the perturbation bound\n",
    "    :param perturb_steps: the maximum PGD step\n",
    "    :param tau: the step controlling how early we should stop interations when wrong adv data is found\n",
    "    :param randominit_type: To decide the type of random inirialization (random start for searching adv data)\n",
    "    :param rand_init: To decide whether to initialize adversarial sample with random noise (random start for searching adv data)\n",
    "    :param omega: random sample parameter for adv data generation (this is for escaping the local minimum.)\n",
    "    :return: output_adv (friendly adversarial data) output_target (targets), output_natural (the corresponding natrual data), count (average backword propagations count)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    K = perturb_steps\n",
    "    count = 0\n",
    "    output_target = []\n",
    "    output_adv = []\n",
    "    output_natural = []\n",
    "\n",
    "    control = (torch.ones(len(target)) * tau).cuda()\n",
    "\n",
    "    # Initialize the adversarial data with random noise\n",
    "    if rand_init:\n",
    "        if randominit_type == \"normal_distribution_randominit\":\n",
    "            iter_adv = data.detach() + 0.001 * torch.randn(data.shape).cuda().detach()\n",
    "            iter_adv = torch.clamp(iter_adv, 0.0, 1.0)\n",
    "        if randominit_type == \"uniform_randominit\":\n",
    "            iter_adv = data.detach() + torch.from_numpy(np.random.uniform(-epsilon, epsilon, data.shape)).float().cuda()\n",
    "            iter_adv = torch.clamp(iter_adv, 0.0, 1.0)\n",
    "    else:\n",
    "        iter_adv = data.cuda().detach()\n",
    "\n",
    "    iter_clean_data = data.cuda().detach()\n",
    "    iter_target = target.cuda().detach()\n",
    "    output_iter_clean_data = model(data)\n",
    "\n",
    "    while K>0:\n",
    "        iter_adv.requires_grad_()\n",
    "        output = model(iter_adv)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        output_index = []\n",
    "        iter_index = []\n",
    "\n",
    "        # Calculate the indexes of adversarial data those still needs to be iterated\n",
    "        for idx in range(len(pred)):\n",
    "            if pred[idx] != iter_target[idx]:\n",
    "                if control[idx] == 0:\n",
    "                    output_index.append(idx)\n",
    "                else:\n",
    "                    control[idx] -= 1\n",
    "                    iter_index.append(idx)\n",
    "            else:\n",
    "                iter_index.append(idx)\n",
    "\n",
    "        # Add adversarial data those do not need any more iteration into set output_adv\n",
    "        if len(output_index) != 0:\n",
    "            if len(output_target) == 0:\n",
    "                # incorrect adv data should not keep iterated\n",
    "                output_adv = iter_adv[output_index].cuda()\n",
    "                output_natural = iter_clean_data[output_index].cuda()\n",
    "                output_target = iter_target[output_index].reshape(-1).cuda()\n",
    "            else:\n",
    "                # incorrect adv data should not keep iterated\n",
    "                output_adv = torch.cat((output_adv, iter_adv[output_index].cuda()), dim=0)\n",
    "                output_natural = torch.cat((output_natural, iter_clean_data[output_index].cuda()), dim=0)\n",
    "                output_target = torch.cat((output_target, iter_target[output_index].reshape(-1).cuda()), dim=0)\n",
    "\n",
    "        # calculate gradient\n",
    "        model.zero_grad()\n",
    "        with torch.enable_grad():\n",
    "            if loss_fn == \"cent\":\n",
    "                loss_adv = nn.CrossEntropyLoss(reduction='mean')(output, iter_target)\n",
    "            if loss_fn == \"kl\":\n",
    "                criterion_kl = nn.KLDivLoss(size_average=False).cuda()\n",
    "                loss_adv = criterion_kl(F.log_softmax(output, dim=1),F.softmax(output_iter_clean_data, dim=1))\n",
    "        loss_adv.backward(retain_graph=True)\n",
    "        grad = iter_adv.grad\n",
    "\n",
    "        # update iter adv\n",
    "        if len(iter_index) != 0:\n",
    "            control = control[iter_index]\n",
    "            iter_adv = iter_adv[iter_index]\n",
    "            iter_clean_data = iter_clean_data[iter_index]\n",
    "            iter_target = iter_target[iter_index]\n",
    "            output_iter_clean_data = output_iter_clean_data[iter_index]\n",
    "            grad = grad[iter_index]\n",
    "            eta = step_size * grad.sign()\n",
    "\n",
    "            iter_adv = iter_adv.detach() + eta + omega * torch.randn(iter_adv.shape).detach().cuda()\n",
    "            iter_adv = torch.min(torch.max(iter_adv, iter_clean_data - epsilon), iter_clean_data + epsilon)\n",
    "            iter_adv = torch.clamp(iter_adv, 0, 1)\n",
    "            count += len(iter_target)\n",
    "        else:\n",
    "            output_adv = output_adv.detach()\n",
    "            return output_adv, output_target, output_natural, count\n",
    "        K = K-1\n",
    "\n",
    "    if len(output_target) == 0:\n",
    "        output_target = iter_target.reshape(-1).squeeze().cuda()\n",
    "        output_adv = iter_adv.cuda()\n",
    "        output_natural = iter_clean_data.cuda()\n",
    "    else:\n",
    "        output_adv = torch.cat((output_adv, iter_adv), dim=0).cuda()\n",
    "        output_target = torch.cat((output_target, iter_target.reshape(-1)), dim=0).squeeze().cuda()\n",
    "        output_natural = torch.cat((output_natural, iter_clean_data.cuda()),dim=0).cuda()\n",
    "    output_adv = output_adv.detach()\n",
    "    return output_adv, output_target, output_natural, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab09249-761d-49d3-8861-1b938ad03f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = input_shape  # Replace with your input shape\n",
    "output_size = output_shape  # Replace with your output shape\n",
    "model = DNNModel(input_size=input_size, output_size=output_size).to(device)\n",
    "\n",
    "# Compile model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping variables\n",
    "min_delta = 0.001\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_loss = float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30ee531-0aed-41ba-8f16-98bf9070b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_tau(epoch):\n",
    "    tau = 0\n",
    "    if epoch <= 4:\n",
    "        tau = 0\n",
    "    elif epoch <= 8:\n",
    "        tau = 1\n",
    "    else:\n",
    "        tau = 2\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9690378-faf1-4f6a-abba-5a83756dc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MART_loss(adv_logits, natural_logits, target, beta):\n",
    "    # Based on the repo MART https://github.com/YisenWang/MART\n",
    "    kl = nn.KLDivLoss(reduction='none')\n",
    "    batch_size = len(target)\n",
    "    adv_probs = F.softmax(adv_logits, dim=1)\n",
    "    tmp1 = torch.argsort(adv_probs, dim=1)[:, -2:]\n",
    "    new_y = torch.where(tmp1[:, -1] == target, tmp1[:, -2], tmp1[:, -1])\n",
    "    loss_adv = F.cross_entropy(adv_logits, target) + F.nll_loss(torch.log(1.0001 - adv_probs + 1e-12), new_y)\n",
    "    nat_probs = F.softmax(natural_logits, dim=1)\n",
    "    true_probs = torch.gather(nat_probs, 1, (target.unsqueeze(1)).long()).squeeze()\n",
    "    loss_robust = (1.0 / batch_size) * torch.sum(\n",
    "        torch.sum(kl(torch.log(adv_probs + 1e-12), nat_probs), dim=1) * (1.0000001 - true_probs))\n",
    "    loss = loss_adv + float(beta) * loss_robust\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f71f5c04-8053-4612-8010-05453cd87061",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m tau \u001b[38;5;241m=\u001b[39m adjust_tau(epoch)\n\u001b[0;32m---> 42\u001b[0m output_adv, output_target, output_natural, count \u001b[38;5;241m=\u001b[39m \u001b[43mearlystop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.007\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.031\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                                         \u001b[49m\u001b[43mrandominit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muniform_randominit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrand_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# outputs = model(output_adv)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# loss = nn.CrossEntropyLoss(reduction='mean')(outputs, output_target)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m adv_logits \u001b[38;5;241m=\u001b[39m model(output_adv)\n",
      "Cell \u001b[0;32mIn[12], line 78\u001b[0m, in \u001b[0;36mearlystop\u001b[0;34m(model, data, target, step_size, epsilon, perturb_steps, tau, randominit_type, loss_fn, rand_init, omega)\u001b[0m\n\u001b[1;32m     76\u001b[0m         criterion_kl \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mKLDivLoss(size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     77\u001b[0m         loss_adv \u001b[38;5;241m=\u001b[39m criterion_kl(F\u001b[38;5;241m.\u001b[39mlog_softmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),F\u001b[38;5;241m.\u001b[39msoftmax(output_iter_clean_data, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 78\u001b[0m \u001b[43mloss_adv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m grad \u001b[38;5;241m=\u001b[39m iter_adv\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# update iter adv\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    bp_count = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        tau = adjust_tau(epoch)\n",
    "        output_adv, output_target, output_natural, count = earlystop(model, data, target, step_size=0.007,\n",
    "                                                                     epsilon=0.031, perturb_steps=3, tau=tau,\n",
    "                                                                     randominit_type=\"uniform_randominit\", loss_fn='cent', rand_init=True, omega=0.001)\n",
    "        bp_count += count\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(output_adv)\n",
    "\n",
    "        adv_logits = model(output_adv)\n",
    "        natural_logits = model(output_natural)\n",
    "\n",
    "        # calculate MART adversarial training loss\n",
    "        loss = MART_loss(adv_logits, natural_logits, output_target, 6)\n",
    "        \n",
    "        # loss = nn.CrossEntropyLoss(reduction='mean')(outputs, output_target)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    bp_count_avg = bp_count / len(train_loader.dataset)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            tau = adjust_tau(epoch)\n",
    "            output_adv, output_target, output_natural, count = earlystop(model, data, target, step_size=0.007,\n",
    "                                                                     epsilon=0.031, perturb_steps=3, tau=tau,\n",
    "                                                                     randominit_type=\"uniform_randominit\", loss_fn='cent', rand_init=True, omega=0.001)\n",
    "            \n",
    "            # outputs = model(output_adv)\n",
    "            # loss = nn.CrossEntropyLoss(reduction='mean')(outputs, output_target)\n",
    "            adv_logits = model(output_adv)\n",
    "            natural_logits = model(output_natural)\n",
    "    \n",
    "            # calculate MART adversarial training loss\n",
    "            loss = MART_loss(adv_logits, natural_logits, output_target, 6)\n",
    "            \n",
    "            val_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "    avg_val_loss = val_train_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping check using min_delta\n",
    "    if best_loss - avg_val_loss > min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8ba48-e84d-42c6-b472-b24b2e15feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_performance_metrics(x_test, y_test, model, 'DNN', 'baseline', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0575444-678d-4c09-909d-4ec33601e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9644509055223006\n",
      "Recall: 0.9358974358974359\n",
      "F1 Score: 0.9493944082853705\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999351277638217\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999354286738841\n",
      "\n",
      "Mean FNR: 0.0641025641025641\n",
      "Mean TNR: 0.9999873395837995\n",
      "Mean FPR: 1.2660416200561605e-05\n",
      "Mean TPR: 0.9358974358974359\n",
      "Accuracy: 0.9920671153870317\n",
      "\n",
      "macro\n",
      "Precision: 0.7571890606264821\n",
      "Recall: 0.8245652527135412\n",
      "F1 Score: 0.7607436670863279\n",
      "\n",
      "weighted\n",
      "Precision: 0.9932169588998347\n",
      "Recall: 0.9920671153870317\n",
      "F1 Score: 0.9924448056268179\n",
      "\n",
      "Mean FNR: 0.17543474728645878\n",
      "Mean TNR: 0.9983052703705271\n",
      "Mean FPR: 0.0016947296294729393\n",
      "Mean TPR: 0.8245652527135412\n",
      "Accuracy: 0.9310919653365612\n",
      "\n",
      "macro\n",
      "Precision: 0.3531046177874929\n",
      "Recall: 0.49295636681820937\n",
      "F1 Score: 0.30315484266668663\n",
      "\n",
      "weighted\n",
      "Precision: 0.9305403457340864\n",
      "Recall: 0.9310919653365612\n",
      "F1 Score: 0.9271923677670507\n",
      "\n",
      "Mean FNR: 0.5070436331817907\n",
      "Mean TNR: 0.8955369936321809\n",
      "Mean FPR: 0.1044630063678192\n",
      "Mean TPR: 0.49295636681820937\n",
      "Accuracy: 0.010106679287799526\n",
      "\n",
      "macro\n",
      "Precision: 0.07044711559549406\n",
      "Recall: 0.15937071047877202\n",
      "F1 Score: 0.03599202078836928\n",
      "\n",
      "weighted\n",
      "Precision: 0.05459476009887281\n",
      "Recall: 0.010106679287799526\n",
      "F1 Score: 0.015260992063980577\n",
      "\n",
      "Mean FNR: 0.840629289521228\n",
      "Mean TNR: 0.6689487099718096\n",
      "Mean FPR: 0.33105129002819045\n",
      "Mean TPR: 0.15937071047877202\n",
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9644509055223006\n",
      "Recall: 0.9358974358974359\n",
      "F1 Score: 0.9493944082853705\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999351277638217\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999354286738841\n",
      "\n",
      "Mean FNR: 0.0641025641025641\n",
      "Mean TNR: 0.9999873395837995\n",
      "Mean FPR: 1.2660416200561605e-05\n",
      "Mean TPR: 0.9358974358974359\n",
      "Accuracy: 0.9898933207122005\n",
      "\n",
      "macro\n",
      "Precision: 0.6510811887575372\n",
      "Recall: 0.850352390494647\n",
      "F1 Score: 0.6853325795999707\n",
      "\n",
      "weighted\n",
      "Precision: 0.9915803899024869\n",
      "Recall: 0.9898933207122005\n",
      "F1 Score: 0.9905865319537936\n",
      "\n",
      "Mean FNR: 0.14964760950535305\n",
      "Mean TNR: 0.9961676803427821\n",
      "Mean FPR: 0.003832319657217912\n",
      "Mean TPR: 0.850352390494647\n",
      "Accuracy: 0.9232512261626032\n",
      "\n",
      "macro\n",
      "Precision: 0.30987689720125217\n",
      "Recall: 0.4906583999099759\n",
      "F1 Score: 0.25703341904761223\n",
      "\n",
      "weighted\n",
      "Precision: 0.9140870214246194\n",
      "Recall: 0.9232512261626032\n",
      "F1 Score: 0.9125465957494563\n",
      "\n",
      "Mean FNR: 0.5093416000900242\n",
      "Mean TNR: 0.8689573212562605\n",
      "Mean FPR: 0.13104267874373957\n",
      "Mean TPR: 0.4906583999099759\n",
      "Accuracy: 0.9112388116589111\n",
      "\n",
      "macro\n",
      "Precision: 0.22899740146396302\n",
      "Recall: 0.4216779163034188\n",
      "F1 Score: 0.21444872410070853\n",
      "\n",
      "weighted\n",
      "Precision: 0.885076674802701\n",
      "Recall: 0.9112388116589111\n",
      "F1 Score: 0.8948639969204959\n",
      "\n",
      "Mean FNR: 0.5783220836965812\n",
      "Mean TNR: 0.8395026162801289\n",
      "Mean FPR: 0.16049738371987118\n",
      "Mean TPR: 0.4216779163034188\n",
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9644509055223006\n",
      "Recall: 0.9358974358974359\n",
      "F1 Score: 0.9493944082853705\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999351277638217\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999354286738841\n",
      "\n",
      "Mean FNR: 0.0641025641025641\n",
      "Mean TNR: 0.9999873395837995\n",
      "Mean FPR: 1.2660416200561605e-05\n",
      "Mean TPR: 0.9358974358974359\n",
      "Accuracy: 0.9920671153870317\n",
      "\n",
      "macro\n",
      "Precision: 0.7571890606264821\n",
      "Recall: 0.8245652527135412\n",
      "F1 Score: 0.7607436670863279\n",
      "\n",
      "weighted\n",
      "Precision: 0.9932169588998347\n",
      "Recall: 0.9920671153870317\n",
      "F1 Score: 0.9924448056268179\n",
      "\n",
      "Mean FNR: 0.17543474728645878\n",
      "Mean TNR: 0.9983052703705271\n",
      "Mean FPR: 0.0016947296294729393\n",
      "Mean TPR: 0.8245652527135412\n",
      "Accuracy: 0.9310919653365612\n",
      "\n",
      "macro\n",
      "Precision: 0.3531046177874929\n",
      "Recall: 0.49295636681820937\n",
      "F1 Score: 0.30315484266668663\n",
      "\n",
      "weighted\n",
      "Precision: 0.9305403457340864\n",
      "Recall: 0.9310919653365612\n",
      "F1 Score: 0.9271923677670507\n",
      "\n",
      "Mean FNR: 0.5070436331817907\n",
      "Mean TNR: 0.8955369936321809\n",
      "Mean FPR: 0.1044630063678192\n",
      "Mean TPR: 0.49295636681820937\n",
      "Accuracy: 0.010106679287799526\n",
      "\n",
      "macro\n",
      "Precision: 0.07044711559549406\n",
      "Recall: 0.15937071047877202\n",
      "F1 Score: 0.03599202078836928\n",
      "\n",
      "weighted\n",
      "Precision: 0.05459476009887281\n",
      "Recall: 0.010106679287799526\n",
      "F1 Score: 0.015260992063980577\n",
      "\n",
      "Mean FNR: 0.840629289521228\n",
      "Mean TNR: 0.6689487099718096\n",
      "Mean FPR: 0.33105129002819045\n",
      "Mean TPR: 0.15937071047877202\n"
     ]
    }
   ],
   "source": [
    "epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Iterate over epsilon values\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_BIM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'BIM', epsilon)\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_FGSM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'FGSM', epsilon)\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_PGD_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'PGD', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0fdd08f-c86d-4544-804e-c1bf1c0cf9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/jovyan/Defense/Adversarial_Training/Adversarial_Training_interpolated.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371f08-a89b-47db-9ef5-2ff3a893bec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
