{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddea530e-6be0-41c5-9465-167ce81f9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from art.attacks.evasion import SimBA, SpatialTransformation, DeepFool, BasicIterativeMethod, FastGradientMethod, ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac054ea5-7fdd-4616-856d-8a5f56d2d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = {\n",
    "            \"model\" : '',\n",
    "            \"attack_model\": '',\n",
    "            'epsilon': '',\n",
    "            'Accuracy': '',\n",
    "            'Macro Precision': '',\n",
    "            'Weighted Precision': '',\n",
    "            'Macro Recall': '',\n",
    "            'Weighted Recall': '',\n",
    "            'Macro F1': '',\n",
    "            'Weighted F1': '',\n",
    "            # 'Macro AUC': '',\n",
    "            # 'Weighted AUC': '',\n",
    "            'TPR': '',\n",
    "            'FNR': '',\n",
    "            'TNR': '',\n",
    "            'FPR': '',\n",
    "        }\n",
    "head = pd.DataFrame([head])\n",
    "head.to_csv(\"/home/jovyan/Defense/Friendly_Adversarial_Training/FAT.csv\", mode='a', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e4b2a1-406e-4f69-9a23-4f1a87c2633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(X_test, y_true, model, model_name, attack_name, eps):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(dataset=test_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        probabilities = np.array(probabilities)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "        # macro_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='macro')\n",
    "        # weighted_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='weighted')\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        def calculate_class_metrics_macro(cm, class_index):\n",
    "            TP = cm[class_index, class_index]\n",
    "            FP = cm[:, class_index].sum() - TP\n",
    "            FN = cm[class_index, :].sum() - TP\n",
    "            TN = cm.sum() - (TP + FP + FN)\n",
    "            \n",
    "            TPR = TP / (TP + FN) if (TP + FN) != 0 else 0  \n",
    "            TNR = TN / (TN + FP) if (TN + FP) != 0 else 0  \n",
    "            FPR = FP / (FP + TN) if (FP + TN) != 0 else 0  \n",
    "            FNR = FN / (FN + TP) if (FN + TP) != 0 else 0  \n",
    "            \n",
    "            return TPR, TNR, FPR, FNR\n",
    "            \n",
    "        metrics = np.array([calculate_class_metrics_macro(cm, i) for i in range(num_classes)])\n",
    "        TPR_macro, TNR_macro, FPR_macro, FNR_macro = np.mean(metrics, axis=0)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        print(\"\\nmacro\")\n",
    "        print(f\"Precision: {precision_macro}\\nRecall: {recall_macro}\\nF1 Score: {f1_macro}\")\n",
    "    \n",
    "        print(\"\\nweighted\")\n",
    "        print(f\"Precision: {precision_weighted}\\nRecall: {recall_weighted}\\nF1 Score: {f1_weighted}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Mean FNR: {FNR_macro}\\nMean TNR: {TNR_macro}\\nMean FPR: {FPR_macro}\\nMean TPR: {TPR_macro}\")\n",
    "\n",
    "        new_row = {\n",
    "            \"model\" : model_name,\n",
    "            \"attack_model\" : attack_name,\n",
    "            'epsilon': eps,\n",
    "            'Accuracy': accuracy,\n",
    "            'Macro Precision': precision_macro,\n",
    "            'Weighted Precision': precision_weighted,\n",
    "            'Macro Recall': recall_macro,\n",
    "            'Weighted Recall': recall_weighted,\n",
    "            'Macro F1': f1_macro,\n",
    "            'Weighted F1': f1_weighted,\n",
    "            # 'Macro AUC': macro_auc,\n",
    "            # 'Weighted AUC': weighted_auc,\n",
    "            'TPR': TPR_macro,\n",
    "            'FNR': FNR_macro,\n",
    "            'TNR': TNR_macro,\n",
    "            'FPR': FPR_macro,\n",
    "        }\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        new_row_df.to_csv(\"/home/jovyan/Defense/Friendly_Adversarial_Training/FAT.csv\", mode='a', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc5288c-f704-4c1e-8b20-f7df5fcf0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('/home/jovyan/Wustl_iiot/x_test.npy')\n",
    "x_train = np.load('/home/jovyan/Wustl_iiot/x_train.npy')\n",
    "x_val = np.load('/home/jovyan/Wustl_iiot/x_val.npy')\n",
    "y_test = np.load('/home/jovyan/Wustl_iiot/y_test.npy')\n",
    "y_train = np.load('/home/jovyan/Wustl_iiot/y_train.npy')\n",
    "y_val = np.load('/home/jovyan/Wustl_iiot/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4e66be-bf25-4a39-b3ef-4d0d8b807d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03857e0-e780-4fa8-96f3-e1399d7f2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1]\n",
    "output_shape = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9ac046-77b7-4630-a4a8-fe44438ac801",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884d3dd2-ab96-48dc-85be-71553ba9454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "        self.fc3 = nn.Linear(30, 20)\n",
    "        self.fc4 = nn.Linear(20, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c47fa36-8689-4ec3-b66f-e0fc79b2d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earlystop(model, data, target, step_size, epsilon, perturb_steps,tau,randominit_type,loss_fn,rand_init=True,omega=0):\n",
    "    '''\n",
    "    The implematation of early-stopped PGD\n",
    "    Following the Alg.1 in our FAT paper <https://arxiv.org/abs/2002.11242>\n",
    "    :param step_size: the PGD step size\n",
    "    :param epsilon: the perturbation bound\n",
    "    :param perturb_steps: the maximum PGD step\n",
    "    :param tau: the step controlling how early we should stop interations when wrong adv data is found\n",
    "    :param randominit_type: To decide the type of random inirialization (random start for searching adv data)\n",
    "    :param rand_init: To decide whether to initialize adversarial sample with random noise (random start for searching adv data)\n",
    "    :param omega: random sample parameter for adv data generation (this is for escaping the local minimum.)\n",
    "    :return: output_adv (friendly adversarial data) output_target (targets), output_natural (the corresponding natrual data), count (average backword propagations count)\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    K = perturb_steps\n",
    "    count = 0\n",
    "    output_target = []\n",
    "    output_adv = []\n",
    "    output_natural = []\n",
    "\n",
    "    control = (torch.ones(len(target)) * tau).cuda()\n",
    "\n",
    "    # Initialize the adversarial data with random noise\n",
    "    if rand_init:\n",
    "        if randominit_type == \"normal_distribution_randominit\":\n",
    "            iter_adv = data.detach() + 0.001 * torch.randn(data.shape).cuda().detach()\n",
    "            iter_adv = torch.clamp(iter_adv, 0.0, 1.0)\n",
    "        if randominit_type == \"uniform_randominit\":\n",
    "            iter_adv = data.detach() + torch.from_numpy(np.random.uniform(-epsilon, epsilon, data.shape)).float().cuda()\n",
    "            iter_adv = torch.clamp(iter_adv, 0.0, 1.0)\n",
    "    else:\n",
    "        iter_adv = data.cuda().detach()\n",
    "\n",
    "    iter_clean_data = data.cuda().detach()\n",
    "    iter_target = target.cuda().detach()\n",
    "    output_iter_clean_data = model(data)\n",
    "\n",
    "    while K>0:\n",
    "        iter_adv.requires_grad_()\n",
    "        output = model(iter_adv)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        output_index = []\n",
    "        iter_index = []\n",
    "\n",
    "        # Calculate the indexes of adversarial data those still needs to be iterated\n",
    "        for idx in range(len(pred)):\n",
    "            if pred[idx] != iter_target[idx]:\n",
    "                if control[idx] == 0:\n",
    "                    output_index.append(idx)\n",
    "                else:\n",
    "                    control[idx] -= 1\n",
    "                    iter_index.append(idx)\n",
    "            else:\n",
    "                iter_index.append(idx)\n",
    "\n",
    "        # Add adversarial data those do not need any more iteration into set output_adv\n",
    "        if len(output_index) != 0:\n",
    "            if len(output_target) == 0:\n",
    "                # incorrect adv data should not keep iterated\n",
    "                output_adv = iter_adv[output_index].cuda()\n",
    "                output_natural = iter_clean_data[output_index].cuda()\n",
    "                output_target = iter_target[output_index].reshape(-1).cuda()\n",
    "            else:\n",
    "                # incorrect adv data should not keep iterated\n",
    "                output_adv = torch.cat((output_adv, iter_adv[output_index].cuda()), dim=0)\n",
    "                output_natural = torch.cat((output_natural, iter_clean_data[output_index].cuda()), dim=0)\n",
    "                output_target = torch.cat((output_target, iter_target[output_index].reshape(-1).cuda()), dim=0)\n",
    "\n",
    "        # calculate gradient\n",
    "        model.zero_grad()\n",
    "        with torch.enable_grad():\n",
    "            if loss_fn == \"cent\":\n",
    "                loss_adv = nn.CrossEntropyLoss(reduction='mean')(output, iter_target)\n",
    "            if loss_fn == \"kl\":\n",
    "                criterion_kl = nn.KLDivLoss(size_average=False).cuda()\n",
    "                loss_adv = criterion_kl(F.log_softmax(output, dim=1),F.softmax(output_iter_clean_data, dim=1))\n",
    "        loss_adv.backward(retain_graph=True)\n",
    "        grad = iter_adv.grad\n",
    "\n",
    "        # update iter adv\n",
    "        if len(iter_index) != 0:\n",
    "            control = control[iter_index]\n",
    "            iter_adv = iter_adv[iter_index]\n",
    "            iter_clean_data = iter_clean_data[iter_index]\n",
    "            iter_target = iter_target[iter_index]\n",
    "            output_iter_clean_data = output_iter_clean_data[iter_index]\n",
    "            grad = grad[iter_index]\n",
    "            eta = step_size * grad.sign()\n",
    "\n",
    "            iter_adv = iter_adv.detach() + eta + omega * torch.randn(iter_adv.shape).detach().cuda()\n",
    "            iter_adv = torch.min(torch.max(iter_adv, iter_clean_data - epsilon), iter_clean_data + epsilon)\n",
    "            iter_adv = torch.clamp(iter_adv, 0, 1)\n",
    "            count += len(iter_target)\n",
    "        else:\n",
    "            output_adv = output_adv.detach()\n",
    "            return output_adv, output_target, output_natural, count\n",
    "        K = K-1\n",
    "\n",
    "    if len(output_target) == 0:\n",
    "        output_target = iter_target.reshape(-1).squeeze().cuda()\n",
    "        output_adv = iter_adv.cuda()\n",
    "        output_natural = iter_clean_data.cuda()\n",
    "    else:\n",
    "        output_adv = torch.cat((output_adv, iter_adv), dim=0).cuda()\n",
    "        output_target = torch.cat((output_target, iter_target.reshape(-1)), dim=0).squeeze().cuda()\n",
    "        output_natural = torch.cat((output_natural, iter_clean_data.cuda()),dim=0).cuda()\n",
    "    output_adv = output_adv.detach()\n",
    "    return output_adv, output_target, output_natural, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab09249-761d-49d3-8861-1b938ad03f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = input_shape  # Replace with your input shape\n",
    "output_size = output_shape  # Replace with your output shape\n",
    "model = DNNModel(input_size=input_size, output_size=output_size).to(device)\n",
    "\n",
    "# Compile model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping variables\n",
    "min_delta = 0.001\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_loss = float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30ee531-0aed-41ba-8f16-98bf9070b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_tau(epoch):\n",
    "    tau = 0\n",
    "    if epoch <= 4:\n",
    "        tau = 0\n",
    "    elif epoch <= 8:\n",
    "        tau = 1\n",
    "    else:\n",
    "        tau = 2\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f5c04-8053-4612-8010-05453cd87061",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss = 0.0\n",
    "    bp_count = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        tau = adjust_tau(epoch)\n",
    "        output_adv, output_target, output_natural, count = earlystop(model, data, target, step_size=0.007,\n",
    "                                                                     epsilon=0.031, perturb_steps=10, tau=tau,\n",
    "                                                                     randominit_type=\"uniform_randominit\", loss_fn='cent', rand_init=True, omega=0.001)\n",
    "        bp_count += count\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(output_adv)\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')(outputs, output_target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    bp_count_avg = bp_count / len(train_loader.dataset)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            tau = adjust_tau(epoch)\n",
    "            output_adv, output_target, output_natural, count = earlystop(model, data, target, step_size=0.007,\n",
    "                                                                     epsilon=0.031, perturb_steps=10, tau=tau,\n",
    "                                                                     randominit_type=\"uniform_randominit\", loss_fn='cent', rand_init=True, omega=0.001)\n",
    "            \n",
    "            outputs = model(output_adv)\n",
    "            loss = nn.CrossEntropyLoss(reduction='mean')(outputs, output_target)\n",
    "            val_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "    avg_val_loss = val_train_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping check using min_delta\n",
    "    if best_loss - avg_val_loss > min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8f8ba48-e84d-42c6-b472-b24b2e15feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9702782261407306\n",
      "Recall: 0.937790728688787\n",
      "F1 Score: 0.9513245284545974\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999390530194503\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999356906225392\n",
      "\n",
      "Mean FNR: 0.062209271311213066\n",
      "Mean TNR: 0.9999872865306166\n",
      "Mean FPR: 1.2713469383253956e-05\n",
      "Mean TPR: 0.937790728688787\n"
     ]
    }
   ],
   "source": [
    "calculate_performance_metrics(x_test, y_test, model, 'DNN', 'baseline', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0575444-678d-4c09-909d-4ec33601e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9644509055223006\n",
      "Recall: 0.9358974358974359\n",
      "F1 Score: 0.9493944082853705\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999351277638217\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999354286738841\n",
      "\n",
      "Mean FNR: 0.0641025641025641\n",
      "Mean TNR: 0.9999873395837995\n",
      "Mean FPR: 1.2660416200561605e-05\n",
      "Mean TPR: 0.9358974358974359\n",
      "Accuracy: 0.9920671153870317\n",
      "\n",
      "macro\n",
      "Precision: 0.7571890606264821\n",
      "Recall: 0.8245652527135412\n",
      "F1 Score: 0.7607436670863279\n",
      "\n",
      "weighted\n",
      "Precision: 0.9932169588998347\n",
      "Recall: 0.9920671153870317\n",
      "F1 Score: 0.9924448056268179\n",
      "\n",
      "Mean FNR: 0.17543474728645878\n",
      "Mean TNR: 0.9983052703705271\n",
      "Mean FPR: 0.0016947296294729393\n",
      "Mean TPR: 0.8245652527135412\n",
      "Accuracy: 0.9310919653365612\n",
      "\n",
      "macro\n",
      "Precision: 0.3531046177874929\n",
      "Recall: 0.49295636681820937\n",
      "F1 Score: 0.30315484266668663\n",
      "\n",
      "weighted\n",
      "Precision: 0.9305403457340864\n",
      "Recall: 0.9310919653365612\n",
      "F1 Score: 0.9271923677670507\n",
      "\n",
      "Mean FNR: 0.5070436331817907\n",
      "Mean TNR: 0.8955369936321809\n",
      "Mean FPR: 0.1044630063678192\n",
      "Mean TPR: 0.49295636681820937\n",
      "Accuracy: 0.010106679287799526\n",
      "\n",
      "macro\n",
      "Precision: 0.07044711559549406\n",
      "Recall: 0.15937071047877202\n",
      "F1 Score: 0.03599202078836928\n",
      "\n",
      "weighted\n",
      "Precision: 0.05459476009887281\n",
      "Recall: 0.010106679287799526\n",
      "F1 Score: 0.015260992063980577\n",
      "\n",
      "Mean FNR: 0.840629289521228\n",
      "Mean TNR: 0.6689487099718096\n",
      "Mean FPR: 0.33105129002819045\n",
      "Mean TPR: 0.15937071047877202\n",
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9644509055223006\n",
      "Recall: 0.9358974358974359\n",
      "F1 Score: 0.9493944082853705\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999351277638217\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999354286738841\n",
      "\n",
      "Mean FNR: 0.0641025641025641\n",
      "Mean TNR: 0.9999873395837995\n",
      "Mean FPR: 1.2660416200561605e-05\n",
      "Mean TPR: 0.9358974358974359\n",
      "Accuracy: 0.9898933207122005\n",
      "\n",
      "macro\n",
      "Precision: 0.6510811887575372\n",
      "Recall: 0.850352390494647\n",
      "F1 Score: 0.6853325795999707\n",
      "\n",
      "weighted\n",
      "Precision: 0.9915803899024869\n",
      "Recall: 0.9898933207122005\n",
      "F1 Score: 0.9905865319537936\n",
      "\n",
      "Mean FNR: 0.14964760950535305\n",
      "Mean TNR: 0.9961676803427821\n",
      "Mean FPR: 0.003832319657217912\n",
      "Mean TPR: 0.850352390494647\n",
      "Accuracy: 0.9232512261626032\n",
      "\n",
      "macro\n",
      "Precision: 0.30987689720125217\n",
      "Recall: 0.4906583999099759\n",
      "F1 Score: 0.25703341904761223\n",
      "\n",
      "weighted\n",
      "Precision: 0.9140870214246194\n",
      "Recall: 0.9232512261626032\n",
      "F1 Score: 0.9125465957494563\n",
      "\n",
      "Mean FNR: 0.5093416000900242\n",
      "Mean TNR: 0.8689573212562605\n",
      "Mean FPR: 0.13104267874373957\n",
      "Mean TPR: 0.4906583999099759\n",
      "Accuracy: 0.9112388116589111\n",
      "\n",
      "macro\n",
      "Precision: 0.22899740146396302\n",
      "Recall: 0.4216779163034188\n",
      "F1 Score: 0.21444872410070853\n",
      "\n",
      "weighted\n",
      "Precision: 0.885076674802701\n",
      "Recall: 0.9112388116589111\n",
      "F1 Score: 0.8948639969204959\n",
      "\n",
      "Mean FNR: 0.5783220836965812\n",
      "Mean TNR: 0.8395026162801289\n",
      "Mean FPR: 0.16049738371987118\n",
      "Mean TPR: 0.4216779163034188\n",
      "Accuracy: 0.9999371735643112\n",
      "\n",
      "macro\n",
      "Precision: 0.9644509055223006\n",
      "Recall: 0.9358974358974359\n",
      "F1 Score: 0.9493944082853705\n",
      "\n",
      "weighted\n",
      "Precision: 0.9999351277638217\n",
      "Recall: 0.9999371735643112\n",
      "F1 Score: 0.9999354286738841\n",
      "\n",
      "Mean FNR: 0.0641025641025641\n",
      "Mean TNR: 0.9999873395837995\n",
      "Mean FPR: 1.2660416200561605e-05\n",
      "Mean TPR: 0.9358974358974359\n",
      "Accuracy: 0.9920671153870317\n",
      "\n",
      "macro\n",
      "Precision: 0.7571890606264821\n",
      "Recall: 0.8245652527135412\n",
      "F1 Score: 0.7607436670863279\n",
      "\n",
      "weighted\n",
      "Precision: 0.9932169588998347\n",
      "Recall: 0.9920671153870317\n",
      "F1 Score: 0.9924448056268179\n",
      "\n",
      "Mean FNR: 0.17543474728645878\n",
      "Mean TNR: 0.9983052703705271\n",
      "Mean FPR: 0.0016947296294729393\n",
      "Mean TPR: 0.8245652527135412\n",
      "Accuracy: 0.9310919653365612\n",
      "\n",
      "macro\n",
      "Precision: 0.3531046177874929\n",
      "Recall: 0.49295636681820937\n",
      "F1 Score: 0.30315484266668663\n",
      "\n",
      "weighted\n",
      "Precision: 0.9305403457340864\n",
      "Recall: 0.9310919653365612\n",
      "F1 Score: 0.9271923677670507\n",
      "\n",
      "Mean FNR: 0.5070436331817907\n",
      "Mean TNR: 0.8955369936321809\n",
      "Mean FPR: 0.1044630063678192\n",
      "Mean TPR: 0.49295636681820937\n",
      "Accuracy: 0.010106679287799526\n",
      "\n",
      "macro\n",
      "Precision: 0.07044711559549406\n",
      "Recall: 0.15937071047877202\n",
      "F1 Score: 0.03599202078836928\n",
      "\n",
      "weighted\n",
      "Precision: 0.05459476009887281\n",
      "Recall: 0.010106679287799526\n",
      "F1 Score: 0.015260992063980577\n",
      "\n",
      "Mean FNR: 0.840629289521228\n",
      "Mean TNR: 0.6689487099718096\n",
      "Mean FPR: 0.33105129002819045\n",
      "Mean TPR: 0.15937071047877202\n"
     ]
    }
   ],
   "source": [
    "epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Iterate over epsilon values\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_BIM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'BIM', epsilon)\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_FGSM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'FGSM', epsilon)\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_PGD_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'PGD', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0fdd08f-c86d-4544-804e-c1bf1c0cf9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/jovyan/Defense/Adversarial_Training/Adversarial_Training_interpolated.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371f08-a89b-47db-9ef5-2ff3a893bec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
