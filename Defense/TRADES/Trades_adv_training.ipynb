{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddea530e-6be0-41c5-9465-167ce81f9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from art.attacks.evasion import SimBA, SpatialTransformation, DeepFool, BasicIterativeMethod, FastGradientMethod, ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac054ea5-7fdd-4616-856d-8a5f56d2d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = {\n",
    "            \"model\" : '',\n",
    "            \"attack_model\": '',\n",
    "            'epsilon': '',\n",
    "            'Accuracy': '',\n",
    "            'Macro Precision': '',\n",
    "            'Weighted Precision': '',\n",
    "            'Macro Recall': '',\n",
    "            'Weighted Recall': '',\n",
    "            'Macro F1': '',\n",
    "            'Weighted F1': '',\n",
    "            # 'Macro AUC': '',\n",
    "            # 'Weighted AUC': '',\n",
    "            'TPR': '',\n",
    "            'FNR': '',\n",
    "            'TNR': '',\n",
    "            'FPR': '',\n",
    "        }\n",
    "head = pd.DataFrame([head])\n",
    "head.to_csv(\"/home/jovyan/Defense/TRADES/TRADES.csv\", mode='a', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e4b2a1-406e-4f69-9a23-4f1a87c2633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(X_test, y_true, model, model_name, attack_name, eps):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(dataset=test_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        probabilities = np.array(probabilities)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "        # macro_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='macro')\n",
    "        # weighted_auc = roc_auc_score(label_binarize(all_labels, classes=range(num_classes)), probabilities[:,1], average='weighted')\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        def calculate_class_metrics_macro(cm, class_index):\n",
    "            TP = cm[class_index, class_index]\n",
    "            FP = cm[:, class_index].sum() - TP\n",
    "            FN = cm[class_index, :].sum() - TP\n",
    "            TN = cm.sum() - (TP + FP + FN)\n",
    "            \n",
    "            TPR = TP / (TP + FN) if (TP + FN) != 0 else 0  \n",
    "            TNR = TN / (TN + FP) if (TN + FP) != 0 else 0  \n",
    "            FPR = FP / (FP + TN) if (FP + TN) != 0 else 0  \n",
    "            FNR = FN / (FN + TP) if (FN + TP) != 0 else 0  \n",
    "            \n",
    "            return TPR, TNR, FPR, FNR\n",
    "            \n",
    "        metrics = np.array([calculate_class_metrics_macro(cm, i) for i in range(num_classes)])\n",
    "        TPR_macro, TNR_macro, FPR_macro, FNR_macro = np.mean(metrics, axis=0)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        \n",
    "        print(\"\\nmacro\")\n",
    "        print(f\"Precision: {precision_macro}\\nRecall: {recall_macro}\\nF1 Score: {f1_macro}\")\n",
    "    \n",
    "        print(\"\\nweighted\")\n",
    "        print(f\"Precision: {precision_weighted}\\nRecall: {recall_weighted}\\nF1 Score: {f1_weighted}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"Mean FNR: {FNR_macro}\\nMean TNR: {TNR_macro}\\nMean FPR: {FPR_macro}\\nMean TPR: {TPR_macro}\")\n",
    "\n",
    "        new_row = {\n",
    "            \"model\" : model_name,\n",
    "            \"attack_model\" : attack_name,\n",
    "            'epsilon': eps,\n",
    "            'Accuracy': accuracy,\n",
    "            'Macro Precision': precision_macro,\n",
    "            'Weighted Precision': precision_weighted,\n",
    "            'Macro Recall': recall_macro,\n",
    "            'Weighted Recall': recall_weighted,\n",
    "            'Macro F1': f1_macro,\n",
    "            'Weighted F1': f1_weighted,\n",
    "            # 'Macro AUC': macro_auc,\n",
    "            # 'Weighted AUC': weighted_auc,\n",
    "            'TPR': TPR_macro,\n",
    "            'FNR': FNR_macro,\n",
    "            'TNR': TNR_macro,\n",
    "            'FPR': FPR_macro,\n",
    "        }\n",
    "        new_row_df = pd.DataFrame([new_row])\n",
    "        new_row_df.to_csv(\"/home/jovyan/Defense/TRADES/TRADES.csv\", mode='a', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc5288c-f704-4c1e-8b20-f7df5fcf0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('/home/jovyan/Wustl_iiot/x_test.npy')\n",
    "x_train = np.load('/home/jovyan/Wustl_iiot/x_train.npy')\n",
    "x_val = np.load('/home/jovyan/Wustl_iiot/x_val.npy')\n",
    "y_test = np.load('/home/jovyan/Wustl_iiot/y_test.npy')\n",
    "y_train = np.load('/home/jovyan/Wustl_iiot/y_train.npy')\n",
    "y_val = np.load('/home/jovyan/Wustl_iiot/y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4e66be-bf25-4a39-b3ef-4d0d8b807d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03857e0-e780-4fa8-96f3-e1399d7f2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1]\n",
    "output_shape = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9ac046-77b7-4630-a4a8-fe44438ac801",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884d3dd2-ab96-48dc-85be-71553ba9454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class DNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "        self.fc3 = nn.Linear(30, 20)\n",
    "        self.fc4 = nn.Linear(20, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5f35a4-8402-4f6a-bcec-7288dc61b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNNModel(input_size=input_shape, output_size=output_shape).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping variables\n",
    "min_delta = 0.001\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffab852c-4d7b-4449-b6cf-d7ad427319df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_l2_norm(x):\n",
    "    flattened = x.view(x.unsqueeze(0).shape[0], -1)\n",
    "    return (flattened ** 2).sum(1)\n",
    "\n",
    "\n",
    "def l2_norm(x):\n",
    "    return squared_l2_norm(x).sqrt()\n",
    "\n",
    "def trades_loss(model,\n",
    "                x_natural,\n",
    "                y,\n",
    "                optimizer,\n",
    "                step_size=0.003,\n",
    "                epsilon=0.031,\n",
    "                perturb_steps=10,\n",
    "                beta=1.0,\n",
    "                distance='l_inf'):\n",
    "    # define KL-loss\n",
    "    criterion_kl = nn.KLDivLoss(size_average=False)\n",
    "    model.eval()\n",
    "    batch_size = len(x_natural)\n",
    "    # generate adversarial example\n",
    "    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
    "    if distance == 'l_inf':\n",
    "        for _ in range(perturb_steps):\n",
    "            x_adv.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
    "                                       F.softmax(model(x_natural), dim=1))\n",
    "            grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
    "            x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
    "            x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    elif distance == 'l_2':\n",
    "        delta = 0.001 * torch.randn(x_natural.shape).cuda().detach()\n",
    "        delta = Variable(delta.data, requires_grad=True)\n",
    "\n",
    "        # Setup optimizers\n",
    "        optimizer_delta = optim.SGD([delta], lr=epsilon / perturb_steps * 2)\n",
    "\n",
    "        for _ in range(perturb_steps):\n",
    "            adv = x_natural + delta\n",
    "\n",
    "            # optimize\n",
    "            optimizer_delta.zero_grad()\n",
    "            with torch.enable_grad():\n",
    "                loss = (-1) * criterion_kl(F.log_softmax(model(adv), dim=1),\n",
    "                                           F.softmax(model(x_natural), dim=1))\n",
    "            loss.backward()\n",
    "            # renorming gradient\n",
    "            grad_norms = delta.grad.view(batch_size, -1).norm(p=2, dim=1)\n",
    "            delta.grad.div_(grad_norms.view(-1, 1, 1, 1))\n",
    "            # avoid nan or inf if gradient is 0\n",
    "            if (grad_norms == 0).any():\n",
    "                delta.grad[grad_norms == 0] = torch.randn_like(delta.grad[grad_norms == 0])\n",
    "            optimizer_delta.step()\n",
    "\n",
    "            # projection\n",
    "            delta.data.add_(x_natural)\n",
    "            delta.data.clamp_(0, 1).sub_(x_natural)\n",
    "            delta.data.renorm_(p=2, dim=0, maxnorm=epsilon)\n",
    "        x_adv = Variable(x_natural + delta, requires_grad=False)\n",
    "    else:\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    model.train()\n",
    "\n",
    "    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    # calculate robust loss\n",
    "    logits = model(x_natural)\n",
    "    loss_natural = F.cross_entropy(logits, y)\n",
    "    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
    "                                                    F.softmax(model(x_natural), dim=1))\n",
    "    loss = loss_natural + beta * loss_robust\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d824aac-2798-43f7-bfaf-bccc40831fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/716256 (0%)]\tLoss: 1.701350\n",
      "Train Epoch: 0 [100000/716256 (14%)]\tLoss: 0.016400\n",
      "Train Epoch: 0 [200000/716256 (28%)]\tLoss: 0.000467\n",
      "Train Epoch: 0 [300000/716256 (42%)]\tLoss: 0.000387\n",
      "Train Epoch: 0 [400000/716256 (56%)]\tLoss: 0.003255\n",
      "Train Epoch: 0 [500000/716256 (70%)]\tLoss: 0.001364\n",
      "Train Epoch: 0 [600000/716256 (84%)]\tLoss: 0.000290\n",
      "Train Epoch: 0 [700000/716256 (98%)]\tLoss: 0.000002\n",
      "Epoch 1, Training Loss: 0.0211, Validation Loss: 0.0008, Validation Accuracy: 0.9997\n",
      "Train Epoch: 1 [0/716256 (0%)]\tLoss: 0.000062\n",
      "Train Epoch: 1 [100000/716256 (14%)]\tLoss: 0.000045\n",
      "Train Epoch: 1 [200000/716256 (28%)]\tLoss: 0.000018\n",
      "Train Epoch: 1 [300000/716256 (42%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [400000/716256 (56%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [500000/716256 (70%)]\tLoss: 0.000012\n",
      "Train Epoch: 1 [600000/716256 (84%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [700000/716256 (98%)]\tLoss: 0.000006\n",
      "Epoch 2, Training Loss: 0.0011, Validation Loss: 0.0005, Validation Accuracy: 0.9999\n",
      "Train Epoch: 2 [0/716256 (0%)]\tLoss: 0.000040\n",
      "Train Epoch: 2 [100000/716256 (14%)]\tLoss: 0.000012\n",
      "Train Epoch: 2 [200000/716256 (28%)]\tLoss: 0.000181\n",
      "Train Epoch: 2 [300000/716256 (42%)]\tLoss: 0.000010\n",
      "Train Epoch: 2 [400000/716256 (56%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [500000/716256 (70%)]\tLoss: 0.000116\n",
      "Train Epoch: 2 [600000/716256 (84%)]\tLoss: 0.000027\n",
      "Train Epoch: 2 [700000/716256 (98%)]\tLoss: 0.000000\n",
      "Epoch 3, Training Loss: 0.0008, Validation Loss: 0.0005, Validation Accuracy: 0.9999\n",
      "Train Epoch: 3 [0/716256 (0%)]\tLoss: 0.000039\n",
      "Train Epoch: 3 [100000/716256 (14%)]\tLoss: 0.000011\n",
      "Train Epoch: 3 [200000/716256 (28%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [300000/716256 (42%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [400000/716256 (56%)]\tLoss: 0.000026\n",
      "Train Epoch: 3 [500000/716256 (70%)]\tLoss: 0.000145\n",
      "Train Epoch: 3 [600000/716256 (84%)]\tLoss: 0.000149\n",
      "Train Epoch: 3 [700000/716256 (98%)]\tLoss: 0.000002\n",
      "Epoch 4, Training Loss: 0.0006, Validation Loss: 0.0004, Validation Accuracy: 0.9999\n",
      "Train Epoch: 4 [0/716256 (0%)]\tLoss: 0.004830\n",
      "Train Epoch: 4 [100000/716256 (14%)]\tLoss: 0.000015\n",
      "Train Epoch: 4 [200000/716256 (28%)]\tLoss: 0.000023\n",
      "Train Epoch: 4 [300000/716256 (42%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [400000/716256 (56%)]\tLoss: 0.000451\n",
      "Train Epoch: 4 [500000/716256 (70%)]\tLoss: 0.001829\n",
      "Train Epoch: 4 [600000/716256 (84%)]\tLoss: 0.000105\n",
      "Train Epoch: 4 [700000/716256 (98%)]\tLoss: 0.000002\n",
      "Epoch 5, Training Loss: 0.0005, Validation Loss: 0.0004, Validation Accuracy: 0.9999\n",
      "Train Epoch: 5 [0/716256 (0%)]\tLoss: 0.000094\n",
      "Train Epoch: 5 [100000/716256 (14%)]\tLoss: 0.000032\n",
      "Train Epoch: 5 [200000/716256 (28%)]\tLoss: 0.000004\n",
      "Train Epoch: 5 [300000/716256 (42%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [400000/716256 (56%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [500000/716256 (70%)]\tLoss: 0.001085\n",
      "Train Epoch: 5 [600000/716256 (84%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [700000/716256 (98%)]\tLoss: 0.000046\n",
      "Epoch 6, Training Loss: 0.0005, Validation Loss: 0.0004, Validation Accuracy: 0.9999\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Adversarial training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # calculate robust loss - TRADES loss\n",
    "        loss = trades_loss(model=model,\n",
    "                           x_natural=data,\n",
    "                           y=target,\n",
    "                           optimizer=optimizer,\n",
    "                           step_size=0.01,\n",
    "                           epsilon=0.3,\n",
    "                           perturb_steps=10,\n",
    "                           beta=1.0,\n",
    "\t\t\t   distance='l_inf')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print progress\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_train_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_train_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping check using min_delta\n",
    "    if best_loss - avg_val_loss > min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8f8ba48-e84d-42c6-b472-b24b2e15feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9998869124157602\n",
      "\n",
      "macro\n",
      "Precision: 0.9991141145568859\n",
      "Recall: 0.8888896297876882\n",
      "F1 Score: 0.9350515414481724\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998870923840951\n",
      "Recall: 0.9998869124157602\n",
      "F1 Score: 0.9998782108214375\n",
      "\n",
      "Mean FNR: 0.11111037021231196\n",
      "Mean TNR: 0.9999760596543066\n",
      "Mean FPR: 2.3940345693446644e-05\n",
      "Mean TPR: 0.8888896297876882\n"
     ]
    }
   ],
   "source": [
    "calculate_performance_metrics(x_test, y_test, model, 'DNN', 'baseline', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0575444-678d-4c09-909d-4ec33601e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9998952892738521\n",
      "\n",
      "macro\n",
      "Precision: 0.9990321391555412\n",
      "Recall: 0.8956043956043956\n",
      "F1 Score: 0.9391517005959731\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998954768667886\n",
      "Recall: 0.9998952892738521\n",
      "F1 Score: 0.9998869822159983\n",
      "\n",
      "Mean FNR: 0.1043956043956044\n",
      "Mean TNR: 0.9999779056195572\n",
      "Mean FPR: 2.2094380442736087e-05\n",
      "Mean TPR: 0.8956043956043956\n",
      "Accuracy: 0.999798955405796\n",
      "\n",
      "macro\n",
      "Precision: 0.9961945176476711\n",
      "Recall: 0.8487350814054089\n",
      "F1 Score: 0.9043722046092373\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998012259436583\n",
      "Recall: 0.999798955405796\n",
      "F1 Score: 0.9997822951190745\n",
      "\n",
      "Mean FNR: 0.15126491859459104\n",
      "Mean TNR: 0.9999585579150814\n",
      "Mean FPR: 4.1442084918573476e-05\n",
      "Mean TPR: 0.8487350814054089\n",
      "Accuracy: 0.9935414424111948\n",
      "\n",
      "macro\n",
      "Precision: 0.6239226706862467\n",
      "Recall: 0.7340417425568896\n",
      "F1 Score: 0.6424946244490969\n",
      "\n",
      "weighted\n",
      "Precision: 0.9938725287078848\n",
      "Recall: 0.9935414424111948\n",
      "F1 Score: 0.9936386356001283\n",
      "\n",
      "Mean FNR: 0.26595825744311047\n",
      "Mean TNR: 0.9986509756884828\n",
      "Mean FPR: 0.0013490243115169957\n",
      "Mean TPR: 0.7340417425568896\n",
      "Accuracy: 0.9302500910983318\n",
      "\n",
      "macro\n",
      "Precision: 0.3870031939222329\n",
      "Recall: 0.3672500329683024\n",
      "F1 Score: 0.2466837510954901\n",
      "\n",
      "weighted\n",
      "Precision: 0.9400596710080226\n",
      "Recall: 0.9302500910983318\n",
      "F1 Score: 0.9119055579812592\n",
      "\n",
      "Mean FNR: 0.6327499670316976\n",
      "Mean TNR: 0.8556608828919959\n",
      "Mean FPR: 0.14433911710800412\n",
      "Mean TPR: 0.3672500329683024\n",
      "Accuracy: 0.9998952892738521\n",
      "\n",
      "macro\n",
      "Precision: 0.9990321391555412\n",
      "Recall: 0.8956043956043956\n",
      "F1 Score: 0.9391517005959731\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998954768667886\n",
      "Recall: 0.9998952892738521\n",
      "F1 Score: 0.9998869822159983\n",
      "\n",
      "Mean FNR: 0.1043956043956044\n",
      "Mean TNR: 0.9999779056195572\n",
      "Mean FPR: 2.2094380442736087e-05\n",
      "Mean TPR: 0.8956043956043956\n",
      "Accuracy: 0.9998575934124388\n",
      "\n",
      "macro\n",
      "Precision: 0.9687618376159819\n",
      "Recall: 0.8631612720336301\n",
      "F1 Score: 0.906057557166102\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998514559654146\n",
      "Recall: 0.9998575934124388\n",
      "F1 Score: 0.9998452206289371\n",
      "\n",
      "Mean FNR: 0.1368387279663699\n",
      "Mean TNR: 0.999970665783233\n",
      "Mean FPR: 2.933421676699329e-05\n",
      "Mean TPR: 0.8631612720336301\n",
      "Accuracy: 0.9861656188613337\n",
      "\n",
      "macro\n",
      "Precision: 0.5194731201236664\n",
      "Recall: 0.7641033436883259\n",
      "F1 Score: 0.5403746402775187\n",
      "\n",
      "weighted\n",
      "Precision: 0.9939181618243426\n",
      "Recall: 0.9861656188613337\n",
      "F1 Score: 0.9898945730814454\n",
      "\n",
      "Mean FNR: 0.23589665631167409\n",
      "Mean TNR: 0.9967690000394865\n",
      "Mean FPR: 0.0032309999605135095\n",
      "Mean TPR: 0.7641033436883259\n",
      "Accuracy: 0.9459148157300641\n",
      "\n",
      "macro\n",
      "Precision: 0.3648168161684099\n",
      "Recall: 0.5226712054621274\n",
      "F1 Score: 0.3378387057201759\n",
      "\n",
      "weighted\n",
      "Precision: 0.9607165928198349\n",
      "Recall: 0.9459148157300641\n",
      "F1 Score: 0.9514009240552921\n",
      "\n",
      "Mean FNR: 0.4773287945378725\n",
      "Mean TNR: 0.938540503205008\n",
      "Mean FPR: 0.06145949679499192\n",
      "Mean TPR: 0.5226712054621274\n",
      "Accuracy: 0.9998952892738521\n",
      "\n",
      "macro\n",
      "Precision: 0.9990321391555412\n",
      "Recall: 0.8956043956043956\n",
      "F1 Score: 0.9391517005959731\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998954768667886\n",
      "Recall: 0.9998952892738521\n",
      "F1 Score: 0.9998869822159983\n",
      "\n",
      "Mean FNR: 0.1043956043956044\n",
      "Mean TNR: 0.9999779056195572\n",
      "Mean FPR: 2.2094380442736087e-05\n",
      "Mean TPR: 0.8956043956043956\n",
      "Accuracy: 0.999798955405796\n",
      "\n",
      "macro\n",
      "Precision: 0.9961945176476711\n",
      "Recall: 0.8487350814054089\n",
      "F1 Score: 0.9043722046092373\n",
      "\n",
      "weighted\n",
      "Precision: 0.9998012259436583\n",
      "Recall: 0.999798955405796\n",
      "F1 Score: 0.9997822951190745\n",
      "\n",
      "Mean FNR: 0.15126491859459104\n",
      "Mean TNR: 0.9999585579150814\n",
      "Mean FPR: 4.1442084918573476e-05\n",
      "Mean TPR: 0.8487350814054089\n",
      "Accuracy: 0.9935414424111948\n",
      "\n",
      "macro\n",
      "Precision: 0.6239226706862467\n",
      "Recall: 0.7340417425568896\n",
      "F1 Score: 0.6424946244490969\n",
      "\n",
      "weighted\n",
      "Precision: 0.9938725287078848\n",
      "Recall: 0.9935414424111948\n",
      "F1 Score: 0.9936386356001283\n",
      "\n",
      "Mean FNR: 0.26595825744311047\n",
      "Mean TNR: 0.9986509756884828\n",
      "Mean FPR: 0.0013490243115169957\n",
      "Mean TPR: 0.7340417425568896\n",
      "Accuracy: 0.9302500910983318\n",
      "\n",
      "macro\n",
      "Precision: 0.3870031939222329\n",
      "Recall: 0.3672500329683024\n",
      "F1 Score: 0.2466837510954901\n",
      "\n",
      "weighted\n",
      "Precision: 0.9400596710080226\n",
      "Recall: 0.9302500910983318\n",
      "F1 Score: 0.9119055579812592\n",
      "\n",
      "Mean FNR: 0.6327499670316976\n",
      "Mean TNR: 0.8556608828919959\n",
      "Mean FPR: 0.14433911710800412\n",
      "Mean TPR: 0.3672500329683024\n"
     ]
    }
   ],
   "source": [
    "epsilon_values = [0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Iterate over epsilon values\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_BIM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'BIM', epsilon)\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_FGSM_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'FGSM', epsilon)\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    filename = f'/home/jovyan/Wustl_iiot/transfer_attack/x_test_adv_PGD_eps_{epsilon}.npy'\n",
    "    x_test_adv = np.load(filename)\n",
    "\n",
    "    calculate_performance_metrics(x_test_adv, y_test, model, 'DNN', 'PGD', epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0fdd08f-c86d-4544-804e-c1bf1c0cf9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/jovyan/Defense/TRADES/TRADES.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15605e82-c422-42f8-86c5-db351cfb5e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
